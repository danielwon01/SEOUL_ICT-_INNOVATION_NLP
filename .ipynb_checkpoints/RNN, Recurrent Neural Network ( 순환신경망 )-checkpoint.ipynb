{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3858793",
   "metadata": {},
   "source": [
    "## RNN, Recurrent Neural Network  (순환신경망 )\n",
    "\n",
    "RNN은 인공신경망의 한 종류로, 유닛간의 연결이 순환적인 구조를 가짐에 따라 순차적인 정보를 모델링 할 수 있는 신경망이다. CNN과 같은 다른 신경망들은 메모리가 없으므로, 시계열 데이터 같은 sequential 한 데이터를 처리하기 위해서는 전체 시퀀스를 하나의 벡터로 넣어줘야 한다. 이런 네트워크를 feed forward network(활성화함수를 지닌 값은 오직 출력층 방향으로만 향함)라고 한다. RNN은 이와 반대로 내부 메모리를 가지기 때문에 시퀀스형태의 입력을 처리 할 수 있다. \n",
    "\n",
    "RNN은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징을 갖고 있다.\n",
    "\n",
    "<img src=\"https://wikidocs.net/images/page/22886/rnn_image1_ver2.PNG\" width=\"300\" height=\"300\"/> \n",
    "\n",
    "\n",
    "RNN에서 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 부분(초록색 박스)을 노드셀이라고 한다. 이 셀은 이전의 값을 기억하려고 하는 일종의 메로리 역할을 수행하므로 이를 메모리셀 또는 RNN셀이라고 한다. \n",
    "\n",
    "은닉층 메모리 셀은 각각의 시점(time step)에서 바로 이전 시점에서의 은닉층의 메모리 셀에서 나온 값을 자신의 입력으로 사용하는 재귀적 활동을 한다. 현재 시점 t에서의 메모리 셀이 갖고 있는 값은 과거의 메모리 셀들의 값에 영향을 받는 것임의 의미 \n",
    "\n",
    "\n",
    "<img src=\"https://wikidocs.net/images/page/22886/rnn_image2_ver3.PNG\" width=\"500\" height=\"300\"/> \n",
    "메모리 셀이 출력층 방향 또는 다음 시점인 t+1의 자신에게 보내는 값을 은닉상태(hidden state)라고 한다. t 시점의 메모리 셀은 t-1 시점의 메모리 셀이 보낸 은닉 상태값을 t 시점의 은닉 상태 계산을 위한 입력값으로 사용\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1125161b",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbJ21vM%2FbtqP9sUwff4%2FdB5MYlreaFqOBl3KcHKMU0%2Fimg.png\" width=\"500\" height=\"300\"/> \n",
    "\n",
    "ono to one 모델은 입력과 출력이 1개인 것으로 대표적으로 vanilla Neural Network가 있다.   \n",
    "\n",
    "ono to many 모델은 입력은 1개, 출력은 여러개인 방법으로 대표적인 예로 image captioning(이미지를 단어로 설명하는 기술)이 있다. input으로 이미지가 들어가며, 이미지 내부의 물체들에 대해 여러개의 단어로 변환하여 설명하는 방식으로 진행됨  \n",
    "\n",
    "many to one는 입력이 여러개, 출력은 하나인 방법이다. 대표적인 예로 감성 분류를 들 수 있다. 일련의 텍스트들인 input으로 들어오고 이 것들이 긍정인지 부정인지 분류를 통해 output으로 나오게 된다.   \n",
    "\n",
    "many to many는 두 가지로 나눠지는 데, 전자의 대표적인 것은 번역(Machine Translation)이 있다. 나라마다 언어의 길이나 크기가 다르기 때문에 입력의 크기와 출력의 크기가 다를 수 있다. 후자의 경우 비디오 분류(Video classification)를 예로 들 수 있다.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea98596",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FXEmoB%2Fbtq2iWJ4jK9%2FHnCtYbogVuJM2ssAgPXCq0%2Fimg.png\" width=\"500\" height=\"300\"/> \n",
    "\n",
    "RNN은 과거의 정보를 현재에 반영해 학습하도록 설계 되었다. 이 방법을 통해 시간순서로 나열된 데이터를 학습한다. 파라미터w가 있는 함수안에 hiddenstate에 있던 이전 값과 현재 인풋 x를 통해 현재 타임 스텝인 h가 갱신된다.  \n",
    "\n",
    "\n",
    "<img src=\"https://wikidocs.net/images/page/22886/rnn_image4_ver2.PNG\" width=\"100\" height=\"300\"/> \n",
    "은닉층 메모리 셀은 $h_t$를 계산하기 위해서 총 두개의 가중치를 가진. 하나는 입력층을 위한 가중치 $W_x$이고, 하나는 이전 시점t-1의 은닉 상태인 $h_t-1$을 위한 가중치 $W_h$이다. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FYJIwM%2Fbtq2s0jGNjW%2F5cfgVO7tXKzvv0DXeArkVK%2Fimg.png\" width=\"500\" height=\"300\"/> \n",
    "\n",
    "\n",
    "\n",
    "주의해야 할 점은 각 시점에서 계산 마다 같은 함수 f와 같은 가중치 w를 사용한다는 것이다. \n",
    "\n",
    "Vanilla RNN에서는 tanh함수를 사용했다. tanh함수는 비선형성을 위해 적용하게 되었는데, 선형함수를 사용하게 되면, 층을 거치면서 크게 변화가 발생하지 않는다. tanh함수는 RNN의 gradient가 최대한 오래 유지할 수 있도록 해주는 역할로 사용된다. \n",
    "\n",
    "$Whh$ 는 h와 h 사이의 파라미터, $Wxh$ 는 x에서 h 사이의 파라미터, $Why$는 h에서 y 사이의 파라미터로 사용됩니다. 여기에 있는 파라미터들은 위에서도 언급했듯이 모든 타임 스텝에서 동일한 값을 가집니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db24b319",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://wikidocs.net/images/page/22886/rnn_images4-5.PNG\" width=\"500\" height=\"300\"/> \n",
    "배치 크기가 1이고 d와 $D_h$모두 4로 가정 \n",
    "\n",
    "$h_t$를 계산하기 위해 활성화 함수로 주로 tanh가 사용된. 각각의 가중치 $W_x$,$W_h$,$W_y$의 값은 하나의 층에서는 모든 시점에서 값을 동일하게 공유한다. \n",
    "하지만 은닉층이 2개 이상일 경우에는 각 은닉층에서의 가중치는 서로 다르다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204c0eb",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbQpybd%2Fbtq2tsUyTv3%2Fq1tuYvXrjgSxRgZvRx8Gxk%2Fimg.png\" width=\"500\" height=\"300\"/> \n",
    "\n",
    "계산 그래프에서 같은 노드를 여러번 재사용하면, 역방향 전파동안 d손실과 dW를 계산할 때 경사를 W 행렬로 결국 더하게 된다.모델에 대한 역전파를 생각한다면, 이 타임 스텝 각각으로부터 흐르는 W에 대한 별도의 경사가 있을 거고 W의 최종 경사는 타임스탭 각각 마다의 경사를 모두 더한 것이 될 것이다. 이 전체 훈련 과정을 위한 최종 손실이 개별 손실들의 합이 될 것이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90de51",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FqQZMJ%2Fbtq2qwX2qZa%2FkVi3K4qZCa4LiJVJW8VaR1%2Fimg.png\" width=\"500\" height=\"300\"/> \n",
    "\n",
    "'hellow'라는 sequence data 주고, 예측하는 예시 이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8fd022",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcoMBlj%2Fbtq2s3HSSNi%2F1KwfzhAdkk7XKo6lPc3fu1%2Fimg.png\" width=\"500\" height=\"300\"/> \n",
    "\n",
    "RNN은 순차적으로 한단계씩 진행해야 한다. 모든 sequence를 다 진행하고 난 후 각 단계의 loss를 더해 최종 Loss를 구한 뒤 backpropagation을 진행하는데 이것을  Backpropagation through time이라고 한다. 이 때는 forward pass도 한단계씩 RNN hidden layer를 업데이트하고 loss를 구하게 되고, backward pass 또한 한단계씩 가야 한다. 만약 sequence의 길이가 엄청나게 길다면 시간이 굉장히 오래 걸리는 단점이 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031d079",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbLFCO6%2Fbtq2tk3BYN3%2FfD0JO16K1kl2KOfZ53pdXK%2Fimg.png\" width=\"500\" height=\"300\"/> \n",
    "\n",
    "위의 문제를 해결하기 위해 배치별로 나누어서 학습을 진행하는 방법도 사용, 배치 크기만큼에 loss를 보고 학습을 진행하는 방식이다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acdc332a",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbpaeCp%2Fbtq2uaf5Gsj%2F58Fcd9gU75eZSKPGSg1Ue1%2Fimg.png\" width=\"500\" height=\"300\"/> \n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FmQc7h%2Fbtq2s0yeXKb%2F61s4Q2kGtAecD17CUjjSs1%2Fimg.png\" width=\"500\" height=\"300\"/> \n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FwhTxe%2Fbtq2pliC6KY%2FsSzkqGXrKDXmMp1wj3qjkK%2Fimg.png\" width=\"500\" height=\"300\"/> \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
