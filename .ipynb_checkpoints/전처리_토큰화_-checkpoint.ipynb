{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4daa6ef",
   "metadata": {
    "id": "f4daa6ef"
   },
   "source": [
    "## Preprocessing: Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a73fd8",
   "metadata": {
    "id": "15a73fd8"
   },
   "source": [
    "---\n",
    "- 참고도서\n",
    "  - 김기현의 자연어 처리 딥러닝 캠프-파이토치편(김기현, 한빛미디어)\n",
    "  - Do it! BERT와 GPT로 배우는 자연어처리(이기창, 이지스퍼블리싱)\n",
    "---\n",
    "\n",
    "추가적으로 cs231n 공부 후 정리 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefee9f",
   "metadata": {},
   "source": [
    "전처리의 표준적인 형태는 데이터를 0으로 평균을 맞추고 표준편차로 정규화하는 것이다.  \n",
    "0으로 중심을 맞추는 것은 모든 입력이 양수인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10869d0",
   "metadata": {
    "id": "b10869d0"
   },
   "source": [
    "## 일반적인 전처리 과정 \n",
    "\n",
    "Corpus -> Normalization -> 문장 단위 분절Tokenizing -> 분절 -> 병렬 코퍼스 정렬 -> 서브워드 분절"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f925c7a",
   "metadata": {
    "id": "8f925c7a"
   },
   "source": [
    "### Corpus, 말뭉치 \n",
    "언어의 본질적인 모습을 총체적으로 드러내 보여줄 수 있는 자료의 집합\n",
    "\n",
    "\n",
    "NLP를 위하여 머신러닝/딥러닝을 수행하려면 훈련데이터가 필요하며, 다수의 문장으로 구성된 코퍼스가 훈련데이터로 사용된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc580c2",
   "metadata": {
    "id": "dfc580c2"
   },
   "source": [
    "* Corpus의 종류로는 단일언어, 이중 언어, 다중 언어와 구성 방법에 따른 분류로 병렬 코러스가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350a7ec6",
   "metadata": {
    "id": "350a7ec6"
   },
   "source": [
    "##### 자연어 처리를 위한 파이썬 패키지 nltk를 이용해 corpus 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4bf47b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4bf47b9",
    "outputId": "fd397481-50b3-4fe0-cade-df53ef6e484e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"book\", quiet=True)\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee0a7e4a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee0a7e4a",
    "outputId": "2319817f-d788-48e1-9d35-4dc9ae339e16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저작권이 말소된 문학 작품을 포함하는 gutenberg corpus에는 작품 샘플이 포함 \n",
    "\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f0714a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8f0714a",
    "outputId": "fb738465-ee17-4320-d742-2aef476209e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moby Dick by Herman Melville 1851]\r\n",
      "\r\n",
      "\r\n",
      "ETYMOLOGY.\r\n",
      "\r\n",
      "(Supplied by a Late Consumptive Usher to a Grammar School)\r\n",
      "\r\n",
      "The pale Usher--threadbare in coat, heart, body, and brain; I see him\r\n",
      "now.  He was ever dusting his old lexicons and grammars, with a queer\r\n",
      "handkerchief, mockingly embellished with all the gay flags of all the\r\n",
      "known nations of the world.  He loved to dust his old grammars; it\r\n",
      "somehow mildly reminded him of his mortality.\r\n",
      "\r\n",
      "\"While you take in hand to school others, and to teach them by what\r\n",
      "name a whale-fish is to be called in our tongue leaving out, through\r\n",
      "ignorance, the letter H, which almost alone maketh the signification\r\n",
      "of the word, you deliver that which is not true.\" --HACKLUYT\r\n",
      "\r\n",
      "\"WHALE. ... Sw. and Dan. HVAL.  This animal is named from roundness\r\n",
      "or rolling; for in Dan. HVALT is arched or vaulted.\" --WEBSTER'S\r\n",
      "DICTIONARY\r\n",
      "\r\n",
      "\"WHALE. ... It is more immediately from the Dut. and Ger. WALLEN;\r\n",
      "A.S. WALW-IAN, to roll, to wallow.\" --RICHARDSON'S DICTIONARY\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "moby_dict = nltk.corpus.gutenberg.raw('melville-moby_dick.txt')\n",
    "print(moby_dict[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7197574a",
   "metadata": {
    "id": "7197574a"
   },
   "source": [
    "### 정제(Normalization, 정규화 )\n",
    "\n",
    "텍스트를 사용하기 위한 필수 과정 \n",
    "\n",
    "* 전각 문자제거, 대소문자 통일, 특수기호 제거, 불필요한 단어의 제거(등장빈도거 적거나 길이가 짧은 단어) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8997c1d8",
   "metadata": {
    "id": "8997c1d8"
   },
   "source": [
    "#### 정규표현식 사용\n",
    "\n",
    "- 참고사이트\n",
    "  - http://pythonstudy.xyz/python/article/401-정규-표현식-Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df05a92",
   "metadata": {
    "id": "5df05a92"
   },
   "outputs": [],
   "source": [
    "# Regex를 위한 모듈 임포트\n",
    "import re\n",
    "\n",
    "# 정규 표현식 지정\n",
    "regex1 = r\"([\\w]+\\s*:? \\s*)?\\(?\\+?([0-9]{1,3})?\\-?[0-9]{2,3}(\\)|\\-)?[0-9]{3,4}\\-?[0-9]{4}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49d04638",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49d04638",
    "outputId": "05eda86a-0466-464a-be37-c9c1116ef8ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "032 252-3123\n"
     ]
    }
   ],
   "source": [
    "text = \"문의사항이 있으면 032-252-3123 으로 연락주시기 바랍니다.\"\n",
    " \n",
    "regex = re.compile(r'(\\d{3})-(\\d{3}-\\d{4})')\n",
    "matchobj = regex.search(text)\n",
    "areaCode = matchobj.group(1)\n",
    "num = matchobj.group(2)\n",
    "fullNum = matchobj.group()\n",
    "print(areaCode, num) # 032 232-3245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b6a8be9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b6a8be9",
    "outputId": "14648cb9-bc85-4624-8225-b4b8bb7409bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "032 252-3123\n"
     ]
    }
   ],
   "source": [
    "# 인덱싱 대신 그룹 명을 지정하여 사용할 수도 있음(Named Capturing Group)\n",
    "# Named Capturing Group을 사용하는 방법은 (?P<그룹명>정규식) 와 같이 정규식 표현 앞에 ?P<그룹명>을 사용함\n",
    "# 이후 MatchObject에서 group('그룹명') 을 호출하면 캡쳐된 그룹 값을 얻을 수 있음\n",
    "\n",
    "regex = re.compile(r'(?P<area>\\d{3})-(?P<num>\\d{3}-\\d{4})')\n",
    "matchobj = regex.search(text)\n",
    "areaCode = matchobj.group(\"area\")\n",
    "num = matchobj.group(\"num\")\n",
    "print(areaCode, num)  # 032 232-3245"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39ad809",
   "metadata": {
    "id": "d39ad809"
   },
   "source": [
    "### 문장 단위 분절 \n",
    "\n",
    "분절이란 자연어 처리에 사용되는 데이터 \n",
    "* 입력단위 : 기본적으로 문장 단위, 한 줄에 한 문장 요구 되지만 여러 문장이 한줄 에 있거나, 한 문장이 여러 줄에 걸쳐있는 경우 분절이 요구 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2c4a23",
   "metadata": {
    "id": "af2c4a23"
   },
   "source": [
    "- NLTK (Natural Language Toolkit)\n",
    "  - Punkt Sentence Tokenizer 모듈\n",
    "    - sent_tokenize(): 파이썬에서 문자열로 인식하는 텍스트는 무엇이든지 받아서 문장 별로 토큰화 할 수 있다.\n",
    "    - word_tokenize(): 파이썬에서 문자열로 인식하는 텍스트는 무엇이든지 받아서 단어 별로 토큰화 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb7945c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb7945c4",
    "outputId": "506024d3-d0b5-48da-8ff0-fc2fb851aa92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK의 Punkt sentence tokenizer 모듈 다운로드\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "578a3b45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "578a3b45",
    "outputId": "4313cb9d-7236-403d-c14a-e55dda339938"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"The Matrix is everywhere its all around us, here even in this room. You can see it out your window or on your television. You feel it when you go to work, or go to church or pay your taxes.\"\n",
    "tokens1 = nltk.sent_tokenize(text1)\n",
    "print(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9faf6d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9faf6d3",
    "outputId": "f23858e4-c333-4b3b-c27a-c93a26c54f28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['자연어 처리는 인공지능의 한 줄기 입니다.', '시퀀스 투 시퀀스의 등장 이후로 딥러닝을 활용한 자연어 처리는 새로운 전기를 맞이하게 되었습니다.', '문장을 받아 단순히 수치로 나타내던 시절을 넘어, 원하는대로 문장을 만들어낼 수 있게 된 것입니다.']\n"
     ]
    }
   ],
   "source": [
    "text2 = \"자연어 처리는 인공지능의 한 줄기 입니다. 시퀀스 투 시퀀스의 등장 이후로 딥러닝을 활용한 자연어 처리는 새로운 전기를 맞이하게 되었습니다. 문장을 받아 단순히 수치로 나타내던 시절을 넘어, 원하는대로 문장을 만들어낼 수 있게 된 것입니다.\"\n",
    "tokens2 = nltk.sent_tokenize(text2)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f6f7f8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f6f7f8c",
    "outputId": "c207a8e9-b599-49d7-845a-f237747045fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자연어 처리는 인공지능의 한 줄기 입니다.\n",
      "시퀀스 투 시퀀스의 등장 이후로 딥러닝을 활용한 자연어 처리는 새로운 전기를 맞이하게 되었습니다.\n",
      "문장을 받아 단순히 수치로 나타내던 시절을 넘어, 원하는대로 문장을 만들어낼 수 있게 된 것입니다.\n"
     ]
    }
   ],
   "source": [
    "if text2.strip() != \"\" :\n",
    "    line = re.sub(r'([a-z])\\.([A-Z])', r'\\1. \\2',text2.strip())\n",
    "    sentences  = nltk.sent_tokenize(line)\n",
    "    \n",
    "    for s in sentences :\n",
    "        if s != \"\" :\n",
    "            print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7604b715",
   "metadata": {
    "id": "7604b715"
   },
   "source": [
    "### 분절 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "082d4d2d",
   "metadata": {
    "id": "082d4d2d"
   },
   "outputs": [],
   "source": [
    "moby_dict = nltk.corpus.gutenberg.raw('melville-moby_dick.txt')\n",
    "moby = moby_dict[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d969e470",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d969e470",
    "outputId": "4f708638-4123-45cf-f1d0-9a4d5be0472b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'Moby',\n",
       " 'Dick',\n",
       " 'by',\n",
       " 'Herman',\n",
       " 'Melville',\n",
       " '1851',\n",
       " ']',\n",
       " 'ETYMOLOGY',\n",
       " '.',\n",
       " '(',\n",
       " 'Supplied',\n",
       " 'by',\n",
       " 'a',\n",
       " 'Late',\n",
       " 'Consumptive',\n",
       " 'Usher',\n",
       " 'to',\n",
       " 'a',\n",
       " 'Grammar',\n",
       " 'School',\n",
       " ')',\n",
       " 'The',\n",
       " 'pale',\n",
       " 'Usher',\n",
       " '--',\n",
       " 'threadbare',\n",
       " 'in',\n",
       " 'coat',\n",
       " ',',\n",
       " 'heart',\n",
       " ',',\n",
       " 'body',\n",
       " ',',\n",
       " 'and',\n",
       " 'brain',\n",
       " ';',\n",
       " 'I',\n",
       " 'see',\n",
       " 'him',\n",
       " 'now',\n",
       " '.',\n",
       " 'He',\n",
       " 'was',\n",
       " 'ever',\n",
       " 'dusting',\n",
       " 'his',\n",
       " 'old',\n",
       " 'lexicons',\n",
       " 'and',\n",
       " 'grammars',\n",
       " ',',\n",
       " 'with',\n",
       " 'a',\n",
       " 'queer',\n",
       " 'handkerchief',\n",
       " ',',\n",
       " 'mockingly',\n",
       " 'embellished',\n",
       " 'with']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb6aa3f",
   "metadata": {
    "id": "bfb6aa3f"
   },
   "source": [
    "- 한글을 영문처럼 띄어쓰기로 분절하고자 하면 NLTK에서는 잘 되지 않음\n",
    "  - 토크나이저에 충분한 정보, 데이터가 반영되어 있지 않기 때문\n",
    "  - KoNLPy 등의 한국어 지원 토크나이저가 필요한 이유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a72075f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a72075f7",
    "outputId": "8010e394-d33d-4c42-e4fc-5930f976054b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['자연어',\n",
       " '처리는',\n",
       " '인공지능의',\n",
       " '한',\n",
       " '줄기',\n",
       " '입니다',\n",
       " '.',\n",
       " '시퀀스',\n",
       " '투',\n",
       " '시퀀스의',\n",
       " '등장',\n",
       " '이후로',\n",
       " '딥러닝을',\n",
       " '활용한',\n",
       " '자연어',\n",
       " '처리는',\n",
       " '새로운',\n",
       " '전기를',\n",
       " '맞이하게',\n",
       " '되었습니다',\n",
       " '.',\n",
       " '문장을',\n",
       " '받아',\n",
       " '단순히',\n",
       " '수치로',\n",
       " '나타내던',\n",
       " '시절을',\n",
       " '넘어',\n",
       " ',',\n",
       " '원하는대로',\n",
       " '문장을',\n",
       " '만들어낼',\n",
       " '수',\n",
       " '있게',\n",
       " '된',\n",
       " '것입니다',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4398a",
   "metadata": {
    "id": "19b4398a"
   },
   "source": [
    "### 병렬 코퍼스 정렬 \n",
    "\n",
    "- 주로 CTK(Champollion Tool kit, Perl로 작성됨) 등의 도구를 활용\n",
    "- 직접 구현하려면 많은 시간과 노력을 요구함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b04d9c",
   "metadata": {
    "id": "42b04d9c"
   },
   "source": [
    "### 토큰화(Tokenizing)\n",
    "- 한국어로는 형태소 분석이라고도 부름\n",
    "- 대표적인 Global Tokenizer는 NLTK 등이 있음\n",
    "\n",
    "주어진 문장에서 토큰 단위로 정보를 나누는 작업, 문장 형태의 데이터를 처리하기 위해 제일 처음 수행해야하는 작업이다.\n",
    "\n",
    "* 토큰화 과정 - 문장을 일정한 의미가 있는 가장 작은 단어로 나눈다. 나눠진 단어들의 의미 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eac35fab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eac35fab",
    "outputId": "30fab5dc-d371-4e50-f7a3-ac8ac18a3988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms']\n"
     ]
    }
   ],
   "source": [
    "# 단순 띄어쓰기로 토큰화 \n",
    "\n",
    "text = \"A Dog Run back corner near spare bedrooms\"\n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb260b",
   "metadata": {
    "id": "40bb260b"
   },
   "source": [
    "- 한국어는 조사, 접사 등으로 인해 단순히 띄어쓰기 단위로 나누면 같은 단어가 다른 단어로 인식되어서 단어 집합(vocabulary)의 크기가 불필요하게 커지기 때문에 토큰화 작업이 까다로움."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f7c2c26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1f7c2c26",
    "outputId": "8746384e-d285-4e7b-a213-1b138091da04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['사과의', '놀라운', '효능이라는', '글을', '봤어.', '그래서', '오늘', '사과를', '먹으려고', '했는데', '사과가', '썩어서', '슈퍼에', '가서', '사과랑', '오렌지', '사왔어']\n"
     ]
    }
   ],
   "source": [
    "kor_text = \"사과의 놀라운 효능이라는 글을 봤어. 그래서 오늘 사과를 먹으려고 했는데 사과가 썩어서 슈퍼에 가서 사과랑 오렌지 사왔어\"\n",
    "print(kor_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e47f1888",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e47f1888",
    "outputId": "b5e478b7-5545-4d1d-bbb0-8a88d69d0693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK는', 'Natural', 'Language', 'Toolkit의', '줄임말입니다', '.', 'Python', '프로그래밍', '언어로', '작성된', '영어의', '기호', '및', '통계', '자연', '언어', '처리를', '위한', '라이브러리', '및', '프로그램', '모음입니다']\n",
      "----------------------------------------------------------------------\n",
      "['NLTK는 Natural Language Toolkit의 줄임말입니다.', 'Python 프로그래밍 언어로 작성된 영어의 기호 및 통계 자연 언어 처리를 위한 라이브러리 및 프로그램 모음입니다']\n"
     ]
    }
   ],
   "source": [
    "# nltk 한국어 지원 \n",
    "\n",
    "text = \"NLTK는 Natural Language Toolkit의 줄임말입니다. Python 프로그래밍 언어로 작성된 영어의 기호 및 통계 자연 언어 처리를 위한 라이브러리 및 프로그램 모음입니다\" \n",
    "\n",
    "word_tokens = nltk.word_tokenize(text) \n",
    "print(word_tokens) \n",
    "\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "sent_tokens = nltk.sent_tokenize(text) \n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0249fa3",
   "metadata": {
    "id": "b0249fa3"
   },
   "source": [
    "###  spaCy\n",
    "- Python의 고급 자연어 처리를 위한 라이브러리\n",
    "- 최신 연구를 기반으로 구축되었으며 처음부터 실제 제품에 사용되도록 설계됨\n",
    "- 사전 훈련된 파이프라인과 함께 제공\n",
    "- 60개 이상의 언어에 대한 토큰화 및 훈련을 지원\n",
    "- 태깅, 구문 분석, 명명된 엔터티 인식, 텍스트 분류 등을 위한 최첨단 속도 및 신경망 모델, BERT와 같은 사전 훈련된 변환기를 통한 다중 작업 학습, 생산 준비 교육\n",
    "- 시스템 및 쉬운 모델이 특징\n",
    "- MIT 라이선스에 따라 출시된 상용 오픈 소스 소프트웨어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a598d06a",
   "metadata": {
    "id": "a598d06a"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mGVLmWKWx6E0",
   "metadata": {
    "id": "mGVLmWKWx6E0"
   },
   "source": [
    "* Text: The original word text.\n",
    "* Lemma: The base form of the word.\n",
    "* POS: The simple UPOS part-of-speech tag.\n",
    "* Tag: The detailed part-of-speech tag.\n",
    "* Dep: Syntactic dependency, i.e. the relation * between tokens.\n",
    "* Shape: The word shape – capitalization, punctuation, digits.\n",
    "* is alpha: Is the token an alpha character?\n",
    "* is stop: Is the token part of a stop list, i.e. the most common words of the language?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "-pVXTaWgyIiR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-pVXTaWgyIiR",
    "outputId": "0ce8b5d4-b197-4390-e0b1-9415f81552d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cafbaa60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cafbaa60",
    "outputId": "619a1834-666b-46fe-8eab-5d3ea1d4b8a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mary', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "text  = text = \"Mary, don't slap the green witch\"\n",
    "print([str(token) for token in nlp (text.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e49dc57f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e49dc57f",
    "outputId": "be8db7e0-6214-4a95-87fb-63662d92176e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['애플이 영국의 신생 기업을 10억 달러에 구매를 고려중이다.',\n",
       " '자동 운전 자동차의 손해 배상 책임에 자동차 메이커에 일정한 부담을 요구하겠다.',\n",
       " '자동 배달 로봇이 보도를 주행하는 것을 샌프란시스코시가 금지를 검토중이라고 합니다.',\n",
       " '런던은 영국의 수도이자 가장 큰 도시입니다.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한국어 \n",
    "\n",
    "from spacy.lang.ko.examples import sentences\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31a29a03",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31a29a03",
    "outputId": "c901fe00-8148-4089-898e-4f68f94720e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[애플이, 영국의, 신생, 기업을, 10억, 달러에, 구매를, 고려중이다.]\n"
     ]
    }
   ],
   "source": [
    "text = sentences[0]\n",
    "print([i for i in nlp(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90abd78d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90abd78d",
    "outputId": "b7473f03-c0c2-4ce0-eeac-9261f5421597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "애플이 PROPN compound\n",
      "영국의 PROPN dep\n",
      "신생 PROPN nsubj\n",
      "기업을 PROPN ROOT\n",
      "10억 PROPN compound\n",
      "달러에 PROPN compound\n",
      "구매를 PROPN compound\n",
      "고려중이다. PROPN dobj\n"
     ]
    }
   ],
   "source": [
    "for token in nlp(text):\n",
    "  print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd634255",
   "metadata": {
    "id": "dd634255"
   },
   "source": [
    "#### Kkma\n",
    "- 서울대학교 지능형 데이터 시스템(IDS) 연구소에서 개발함\n",
    "- 자바로 작성된 형태소 분석기 및 자연어 처리 시스템\n",
    "- 참고: Kkma 품사 태그\n",
    "  - NNG: 일반 명사\n",
    "  - JKS: 주격 조사\n",
    "  - JKM: 부사격 조사\n",
    "  - VV: 동사\n",
    "  - EFN: 평서형 종결 어미\n",
    "  - SF: 마침표, 물음표, 느낌표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "PScIYFIdyi-p",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PScIYFIdyi-p",
    "outputId": "cf663d65-2c8f-478d-af50-c9d8e84ef448"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.4 MB 6.1 MB/s \n",
      "\u001b[?25hCollecting JPype1>=0.7.0\n",
      "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
      "\u001b[K     |████████████████████████████████| 448 kB 33.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
      "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.2.0)\n",
      "Installing collected packages: JPype1, konlpy\n",
      "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6221295",
   "metadata": {
    "id": "c6221295"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "424f47a8",
   "metadata": {
    "id": "424f47a8"
   },
   "outputs": [],
   "source": [
    "kkma = Kkma()\n",
    "text  = \"미국 중앙은행(Fed)이 22년 만에 기준금리를 50bp(1bp=0.01%포인트) 인상하는 ‘빅스텝’을 밟았다. Fed의 보유 자산을 줄이는 양적긴축도 다음달부터 시작한다. 41년 만의 최고 수준인 인플레이션과의 전쟁이 본격화했다는 분석이 나온다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff3f6796",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff3f6796",
    "outputId": "134a155b-de34-4215-a053-0f24aeeadc6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['미국', '중앙', '은행', '(', 'Fed', ')', '이', '22', '년', '만', '에', '기준', '금리', '를', '50', 'bp', '(', '1', 'bp', '=', '0.01', '%', '포인트', ')', '인상', '하', '는', '‘', '빅', '스텝', '’', '을', '밟', '았', '다', '.', 'Fed', '의', '보유', '자산', '을', '줄이', '는', '양적', '긴축', '도', '다음달', '부터', '시작하', 'ㄴ다', '.', '41', '년', '만', '의', '최고', '수준', '이', 'ㄴ', '인플레이션', '과', '의', '전쟁', '이', '본격화', '하', '었', '다는', '분석', '이', '나오', 'ㄴ다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 추출\n",
    "\n",
    "morphs = kkma.morphs(text)\n",
    "print(morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e61ccba8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e61ccba8",
    "outputId": "665df4c5-a374-4884-91ef-44ccecc3a581"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('미국', 'NNP'), ('중앙', 'NNG'), ('은행', 'NNG'), ('(', 'SS'), ('Fed', 'OL'), (')', 'SS'), ('이', 'MDT'), ('22', 'NR'), ('년', 'NNM'), ('만', 'NNB'), ('에', 'JKM'), ('기준', 'NNG'), ('금리', 'NNG'), ('를', 'JKO'), ('50', 'NR'), ('bp', 'OL'), ('(', 'SS'), ('1', 'NR'), ('bp', 'OL'), ('=', 'SW'), ('0.01', 'NR'), ('%', 'SW'), ('포인트', 'NNG'), (')', 'SS'), ('인상', 'NNG'), ('하', 'XSV'), ('는', 'ETD'), ('‘', 'SS'), ('빅', 'NNG'), ('스텝', 'NNG'), ('’', 'SS'), ('을', 'NNG'), ('밟', 'VV'), ('았', 'EPT'), ('다', 'EFN'), ('.', 'SF'), ('Fed', 'OL'), ('의', 'JKG'), ('보유', 'NNG'), ('자산', 'NNG'), ('을', 'JKO'), ('줄이', 'VV'), ('는', 'ETD'), ('양적', 'NNG'), ('긴축', 'NNG'), ('도', 'JX'), ('다음달', 'NNG'), ('부터', 'JX'), ('시작하', 'VV'), ('ㄴ다', 'EFN'), ('.', 'SF'), ('41', 'NR'), ('년', 'NNM'), ('만', 'NNB'), ('의', 'JKG'), ('최고', 'NNG'), ('수준', 'NNG'), ('이', 'VCP'), ('ㄴ', 'ETD'), ('인플레이션', 'NNG'), ('과', 'JKM'), ('의', 'JKG'), ('전쟁', 'NNG'), ('이', 'JKS'), ('본격화', 'NNG'), ('하', 'XSV'), ('었', 'EPT'), ('다는', 'ETD'), ('분석', 'NNG'), ('이', 'JKS'), ('나오', 'VV'), ('ㄴ다', 'EFN'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "# 형태소와 품사 태그 추출\n",
    "pos = kkma.pos(text)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "957194ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "957194ee",
    "outputId": "f0c2d16f-ca0c-4154-957a-cd38a56d6871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['미국', '중앙', '중앙은행', '은행', '22', '22년', '년', '만', '기준', '기준금리', '금리', '50', '1', '0.01', '포인트', '인상', '빅', '빅스텝', '스텝', '을', '보유', '자산', '양적', '양적긴축', '긴축', '다음달', '41', '41년', '최고', '수준', '인플레이션', '전쟁', '본격화', '분석']\n"
     ]
    }
   ],
   "source": [
    "# 명사만 추출\n",
    "nouns = kkma.nouns(text)\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4771290b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4771290b",
    "outputId": "7371a100-6114-4688-89c4-ceef02c0b671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘 날씨는 어 때요?', '내일은 덥다 던데.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 분리\n",
    "sentences = \"오늘 날씨는 어때요? 내일은 덥다던데.\"\n",
    "s = kkma.sentences(sentences)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K_Nf6jh50l9P",
   "metadata": {
    "id": "K_Nf6jh50l9P"
   },
   "source": [
    "#### Okt\n",
    "- 스칼라로 작성된 오픈소스 한국어 형태소 분석기\n",
    "- 참고: Okt 품사 태그\n",
    "  - Noun: 명사\n",
    "  - Verb: 동사\n",
    "  - Adjective: 형용사\n",
    "  - Josa: 조사\n",
    "  - Punctuation: 구두점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "QwtnNwEz0mCe",
   "metadata": {
    "id": "QwtnNwEz0mCe"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "RUQFQ-0x0mH8",
   "metadata": {
    "id": "RUQFQ-0x0mH8"
   },
   "outputs": [],
   "source": [
    "okt = Okt() \n",
    "text  = \"미국 중앙은행(Fed)이 22년 만에 기준금리를 50bp(1bp=0.01%포인트) 인상하는 ‘빅스텝’을 밟았다. Fed의 보유 자산을 줄이는 양적긴축도 다음달부터 시작한다. 41년 만의 최고 수준인 인플레이션과의 전쟁이 본격화했다는 분석이 나온다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8SY9EhYd1-y8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8SY9EhYd1-y8",
    "outputId": "4e45d266-9d89-4e6d-9f15-bf04ffa4e497"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['미국', '중앙은행', '(', 'Fed', ')', '이', '22년', '만에', '기준금리', '를', '50', 'bp', '(', '1', 'bp', '=', '0.01%', '포인트', ')', '인상', '하는', '‘', '빅스', '텝', '’', '을', '밟았다', '.', 'Fed', '의', '보유', '자산', '을', '줄이는', '양적', '긴축', '도', '다음', '달', '부터', '시작', '한', '다', '.', '41년', '만의', '최고', '수준', '인', '인플레이션', '과의', '전쟁', '이', '본격', '화했다는', '분석', '이', '나온다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 추출\n",
    "morphs = okt.morphs(text)\n",
    "print(morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ecYWPlE90mNU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecYWPlE90mNU",
    "outputId": "fdbea0d6-a9c4-496a-d852-d4d01c651513"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('미국', 'Noun'), ('중앙은행', 'Noun'), ('(', 'Punctuation'), ('Fed', 'Alpha'), (')', 'Punctuation'), ('이', 'Noun'), ('22년', 'Number'), ('만에', 'Josa'), ('기준금리', 'Noun'), ('를', 'Josa'), ('50', 'Number'), ('bp', 'Alpha'), ('(', 'Punctuation'), ('1', 'Number'), ('bp', 'Alpha'), ('=', 'Punctuation'), ('0.01%', 'Number'), ('포인트', 'Noun'), (')', 'Punctuation'), ('인상', 'Noun'), ('하는', 'Verb'), ('‘', 'Foreign'), ('빅스', 'Noun'), ('텝', 'Noun'), ('’', 'Punctuation'), ('을', 'Josa'), ('밟았다', 'Verb'), ('.', 'Punctuation'), ('Fed', 'Alpha'), ('의', 'Noun'), ('보유', 'Noun'), ('자산', 'Noun'), ('을', 'Josa'), ('줄이는', 'Verb'), ('양적', 'Noun'), ('긴축', 'Noun'), ('도', 'Josa'), ('다음', 'Noun'), ('달', 'Noun'), ('부터', 'Josa'), ('시작', 'Noun'), ('한', 'Josa'), ('다', 'Adverb'), ('.', 'Punctuation'), ('41년', 'Number'), ('만의', 'Josa'), ('최고', 'Noun'), ('수준', 'Noun'), ('인', 'Josa'), ('인플레이션', 'Noun'), ('과의', 'Josa'), ('전쟁', 'Noun'), ('이', 'Josa'), ('본격', 'Noun'), ('화했다는', 'Adjective'), ('분석', 'Noun'), ('이', 'Josa'), ('나온다', 'Verb'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "# 형태소와 품사 태그 추출\n",
    "pos = okt.pos(text)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "-uWd53Hx0mw6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-uWd53Hx0mw6",
    "outputId": "032fd40c-4519-4b57-c8c2-0e05d3d188a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['미국', '중앙은행', '이', '기준금리', '포인트', '인상', '빅스', '텝', '의', '보유', '자산', '양적', '긴축', '다음', '달', '시작', '최고', '수준', '인플레이션', '전쟁', '본격', '분석']\n"
     ]
    }
   ],
   "source": [
    "# 명사만 추출\n",
    "nouns = okt.nouns(text)\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "-zH0em0Q0m2L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-zH0em0Q0m2L",
    "outputId": "21027b21-c904-4853-caf4-20110c26a3e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 날씨가 좋아요 ㅋㅋ\n",
      "['오늘', '오늘 날씨', '날씨']\n"
     ]
    }
   ],
   "source": [
    "# 정규화, 어구 추출\n",
    "text = \"오늘 날씨가 좋아요 ㅋㅋ\"\n",
    "print(okt.normalize(text))\n",
    "print(okt.phrases(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "__XctpwR0m8L",
   "metadata": {
    "id": "__XctpwR0m8L"
   },
   "source": [
    "###  사용자 사전 구축\n",
    "\n",
    "자연어처리에서  많은 시간을 소요하는 작업\n",
    "자연어처리는 사전을 만들면 만들 수록 품질이 좋아지게 되는데 이는 현재 존재하는 오픈 품사 사전이 매우 적으며, 개체명과 신조어 같은 것들이 포함이 되어 있지 않기 떄문이다. \n",
    "\n",
    "* Komoran을 이용해서 사전 구축 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fr4ssUGH0nFr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fr4ssUGH0nFr",
    "outputId": "239dbb3d-51fb-4937-cd72-4d9907ee53e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('우리', 'NP'), ('챗봇은', 'NA'), ('엔', 'NNB'), ('엘', 'NNP'), ('피', 'NNG'), ('를', 'JKO'), ('좋아하', 'VV'), ('아', 'EC')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()\n",
    "text = \"우리 챗봇은 엔엘피를 좋아해\"\n",
    "pos = komoran.pos(text)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "arM46e_z0nLi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "arM46e_z0nLi",
    "outputId": "dbcc8d7e-0bd9-4d30-baae-b1896f1cfaee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('우리', 'NP'), ('챗봇은', 'NA'), ('엔', 'NNB'), ('엘', 'NNP'), ('피', 'NNG'), ('를', 'JKO'), ('좋아하', 'VV'), ('아', 'EC')]\n"
     ]
    }
   ],
   "source": [
    "filepath = \"user_dic.txt\"\n",
    "komoran = Komoran(userdic=filepath)\n",
    "pos = komoran.pos(text)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35a438",
   "metadata": {
    "id": "8d35a438"
   },
   "source": [
    "- 문제점\n",
    "  - 공식 가이드대로 작성하여도 동작하지 않음\n",
    "  - 각 패키지별로 사용자 사전을 추가하는 기능이 제공되지만 오류로 인해 동작하지 않거나 설치할 수 없는 경우가 많음\n",
    "  - 주로 Java, C++ 등을 기반으로 개발되었고 파이썬으로 사용할 수 있도록 기능 지원을 추가한 형태여서 파이썬 작업만으로는 제대로 사용하지 못하는 경우가 많음"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "전처리 토큰화 .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
