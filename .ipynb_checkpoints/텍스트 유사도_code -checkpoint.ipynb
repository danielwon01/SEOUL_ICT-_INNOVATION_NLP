{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d89a38c5",
   "metadata": {},
   "source": [
    "## 텍스트 유사도 \n",
    "\n",
    "\n",
    "---\n",
    "- 참고도서\n",
    "  - 처음 배우는 딥러닝 챗봇 (조경래 지음 | 한빛미디어)\n",
    "  - 밑바닥부터 시작하는 딥러닝 2 (사이토 고키 지음/개앞맵시 옮김 | 한빛미디어)\n",
    "  - 자연어 처리 바이블 (임희석, 고려대학교 자연어처리연구실 지음 | 휴먼싸이언스)\n",
    "  - 텐서플로 2와 머신러닝으로 시작하는 자연어 처리  (전창욱, 최태균, 조종현, 신성진 지음 | 위키북스)\n",
    "- 참고사이트\n",
    "  - https://koreapy.tistory.com/530\n",
    "  - https://namu.wiki/w/노름(수학)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2578c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c565e",
   "metadata": {},
   "source": [
    "### Norm 기반 유사도 \n",
    "\n",
    " Norm \n",
    "  - 선형대수에서의 Norm은 공간에 있는 모든 벡터의 총 길이\n",
    "  - 거리의 일반화가 거리함수(distance function, 혹은 metric)라면 노름은 크기의 일반화를 가리킨다.\n",
    "  - 실수의 크기(절댓값)를 $\\lvert x \\rvert$로 표현하듯, 벡터의 크기(노름)은 일반적으로 $\\lVert{x}\\rVert$라고 표현한다.\n",
    "  - 저자에 따라서는 유클리드 공간의 노름을 $\\lvert{x}\\rvert$로 쓰기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e25b3d",
   "metadata": {},
   "source": [
    "- Norm의 종류\n",
    "  - L0 Norm\n",
    "    - 실제 거리를 나타내는 Norm은 아님\n",
    "    - 여러 개의 벡터 구성 요소 중에서 몇 개의 요소가 변화했는지, 올바르게 구성되어 있는지에 대한 확인을 위해 사용됨\n",
    "    - 벡터에서 0이 아닌 요소의 총 수에 해당함\n",
    "    - 예: 벡터 (0,0), (0,2)의 L0 Norm은 0이 아닌 요소가 하나뿐이므로 1\n",
    "<br><br>\n",
    "  - L1 Norm\n",
    "    - 맨해튼 거리(Manhattan Distance/Norm) 또는 택시 거리(Taxicab Norm)라고 부르기도 함\n",
    "    - 두 벡터의 각 차원별 값의 차이의절대값을 모두 합한 값이며, 일반적인 거리가 아닌 특이한 거리을 표시하는 특징을 가짐\n",
    "    - 예: 좌표공간에 A(3,4)라는 점이 있다면 일반적인 (원점으로부터의)거리는\n",
    "$\\sqrt{{3^2}+{4^2}}=5$ 이지만 L1 Norm은 $3+4=7$임\n",
    "    - 계산 방법: $\\lVert{x}\\rVert_1=\\lvert{3}\\rvert+\\lvert{4}\\rvert=7$\n",
    "    - <img src='https://drive.google.com/uc?export=download&id=1GUXEIgE_5gWzWnhtQcaTJ9GJdeoeR2nh'>\n",
    "$$\n",
    "\\text{d}_{\\text{L1}}(w,v)=\\sum_{i=1}^d{|w_i-v_i|},\\text{ where }w,v\\in\\mathbb{R}^d.\n",
    "$$\n",
    "<br><br>\n",
    "  - L2 Norm\n",
    "    - 가장 널리 사용되는 표준 거리로 유클리드 공간(Euclidean Space)에 존재하기때문에 유클리디안 거리(Euclidean Distance)라고도 부름\n",
    "    - 잘 알려진 피타고라스의 정리에 따른 거리 계산 방법\n",
    "    - 한 지점에서 다른 지점으로 이동할 수 있는 최단 거리\n",
    "    - 계산 방법: $\\lVert{x}\\rVert_2=\\sqrt{{\\lvert{3}\\rvert}^2+{\\lvert{4}\\rvert}^2}=\\sqrt{9+16}=\\sqrt{25}=5$\n",
    "    \n",
    "    - <img src='https://drive.google.com/uc?export=download&id=1OdiWb0n8IiweZcLicjlNasjM7le7oNYZ'>\n",
    "    - 벡터의 각 구성요소가 제곱 값으로 계산되므로, 특이값들이 일반적인 값보다 더 많은 가중치를 가지게 되어 결과를 왜곡하기 쉬움 → 일부 상황에서는 L1 Norm을 사용하게 되는 이유가 됨\n",
    "    - 선형회귀의 선형성을 이탈하는 데이터 점(특이점)들에 대한 왜곡이 선형회귀식에서는 매우 큰 오류로 작용하게 되므로 그러한 경우에 L1 Norm을 사용하도록 설정하고 있지만 실무에서는 그냥 특이값은 제거해 버리는 편임\n",
    "$$\n",
    "\\text{d}_{\\text{L2}}(w,v)=\\sqrt{\\sum_{i=1}^d{(w_i-v_i)^2}},\\text{ where }w,v\\in\\mathbb{R}^d.\n",
    "$$\n",
    "<br><br>\n",
    "  - L-infinity Norm ($L_∞$ Norm)\n",
    "    - 벡터공간에서 가장 큰 크기를 제공하는 Norm\n",
    "    - 머신러닝, 딥러닝 분야에서는 그다지 취급하지 않음\n",
    "$$\n",
    "d_{\\infty}(w,v)=\\max(|w_1-v_1|,|w_2-v_2|,\\cdots,|w_d-v_d|),\\text{ where }w,v\\in\\mathbb{R}^d\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae7c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 Distance \n",
    "\n",
    "def L1_Distance (x1 ,x2) :\n",
    "    return ((x1 - x2).abs().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2af71d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Distance\n",
    "def L2_Distance(x1, x2):\n",
    "    return ((x1 - x2)**2).sum()**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4070a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L-Infinity Distance\n",
    "\n",
    "def infinity_distance(x1, x2):\n",
    "    return ((x1 - x2).abs()).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d78ddc",
   "metadata": {},
   "source": [
    "### n-gram 유사도 \n",
    "\n",
    "- n-gram\n",
    "  - 주어진 문장에서 n개의 연속적인 단어 시퀀스(나열)를 의미\n",
    "  - n-gram은 문장에서 n개의 단어를 토큰으로 사용함\n",
    "  - 이웃한 단어의 출현 횟수를 통계적으로 표현해 텍스트의 유사도를 계산하는 방법\n",
    "  - $\\begin{aligned}Similarity=\\frac{tf(A,B)}{tokens(A)}\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f3e7a1",
   "metadata": {},
   "source": [
    "- n-gram을 이용한 문장 간의 유사도 계산\n",
    "    - 해당 문장을 n-gram으로 토큰 분리\n",
    "    - 단어 문서 행렬(Term-Document Matrix, TDM) 생성\n",
    "    - 두 문장 간 비교, 동일한 단어의 출현빈도를 확률로 계산\n",
    "    - 계산 결과가 1.0에 가까울수록 B가 A과 유사하다고 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d039b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe459bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어절 단위 n-gram ,문장을 이루는 마디\n",
    "\n",
    "def word_ngram(bow, num_gram):\n",
    "    text = tuple(bow)\n",
    "    ngrams = [text[x:x + num_gram] for x in range(0, len(text))]\n",
    "    return tuple(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63e3b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음절 n-gram 분석\n",
    "def phoneme_ngram(bow, num_gram):\n",
    "    sentence = ' '.join(bow)\n",
    "    text = tuple(sentence)\n",
    "    slen = len(text)\n",
    "    ngrams = [text[x:x + num_gram] for x in range(0, slen)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e85576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 계산\n",
    "def similarity(doc1, doc2):\n",
    "    cnt = 0\n",
    "    for token in doc1:\n",
    "        if token in doc2:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt/len(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f26d59c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 정의 \n",
    "\n",
    "sentence1 = \"한미 정상은 이번 회담에서 반도체·배터리·인공지능(AI) 등 핵심·신흥기술 협력과 안전하고 지속 가능하며 회복력 있는 글로벌 공급망을 위해서도 공조하기로 했다.\"\n",
    "sentence2 = '윤 대통령과 바이든 대통령은 세계 최대 반도체 생산시설인 삼성 평택캠퍼스를 함께 시찰하며 반도체 동맹 행보에 나섰다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f91d8793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('미', '정상은'), ('정상은', '이번'), ('이번', '회담'), ('회담', '반도체'), ('반도체', '배터리'), ('배터리', '인공지능'), ('인공지능', '등'), ('등', '핵심'), ('핵심', '신흥'), ('신흥', '기술'), ('기술', '협력'), ('협력', '안전'), ('안전', '지속'), ('지속', '회복'), ('회복', '력'), ('력', '글로벌'), ('글로벌', '공급'), ('공급', '망'), ('망', '공조'), ('공조',))\n",
      "(('윤', '대통령'), ('대통령', '바이든'), ('바이든', '대통령'), ('대통령', '세계'), ('세계', '최대'), ('최대', '반도체'), ('반도체', '생산'), ('생산', '시설'), ('시설', '삼성'), ('삼성', '평택'), ('평택', '캠퍼스'), ('캠퍼스', '시찰'), ('시찰', '반도체'), ('반도체', '동맹'), ('동맹', '행보'), ('행보',))\n"
     ]
    }
   ],
   "source": [
    "# 형태소분석기를 이용해 단어 묶음 리스트 생성\n",
    "\n",
    "komoran = Komoran()\n",
    "\n",
    "bow1 = komoran.nouns(sentence1)\n",
    "bow2 = komoran.nouns(sentence2)\n",
    "\n",
    "doc1 = word_ngram(bow1 , 2)\n",
    "doc2 = word_ngram(bow2, 2)\n",
    "\n",
    "print(doc1)\n",
    "print(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c40b0a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "r1 = similarity(doc1, doc2)\n",
    "print(r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4d870e",
   "metadata": {},
   "source": [
    "### 코사인 유사도 \n",
    "\n",
    "- 두 벡터 사이의 요소별 곱을 사용하여 거리를 계산하는 방법으로 벡터의 내적과 같음\n",
    "- 두 벡터 사이의 방향과 크기를 모두 고려함\n",
    "- 코사인 유사도의 결과가\n",
    "  - 1에 가까울수록 방향은 일치\n",
    "  - 0에 가까울수록 직교\n",
    "  - -1에 가까울수록 반대 방향\n",
    "- 연산에 대한 부하가 큼\n",
    "- 희소벡터일 경우 큰 문제가 발생함\n",
    "  - 윗변이 벡터의 곱으로 표현되므로 0이 들어간 차원이 많으면 해당 차원이 직교하면서 곱의 값이 0이 됨\n",
    "  - 따라서 정확한 유사도 또는 거리를 계산하지 못함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de23374",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{sim}_{\\text{cos}}(w,v)&=\\overbrace{\\frac{w\\cdot v}{|w||v|}}^{\\text{dot product}}\n",
    "=\\overbrace{\\frac{w}{|w|}}^{\\text{unit vector}}\\cdot\\frac{v}{|v|} \\\\\n",
    "&=\\frac{\\sum_{i=1}^{d}{w_iv_i}}{\\sqrt{\\sum_{i=1}^d{w_i^2}}\\sqrt{\\sum_{i=1}^d{v_i^2}}} \\\\\n",
    "\\text{where }&w,v\\in\\mathbb{R}^d\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdfcb0b",
   "metadata": {},
   "source": [
    "- 단어나 문장을 벡터로 표현할 수 있다면 벡터간 거리나 각도를 이용해서 단어, 문장 사이의 유사성을 계산할 수 있음\n",
    "- 코사인 유사도는 벡터의 크기가 중요하지 않을 때 그 거리를 측정하기 위하여 많이 사용됨\n",
    "- 단어들의 출현 빈도를 통해 유사도 계산을 한다면\n",
    "  - 동일한 단어가 많이 포함되어 있을수록 벡터의 크기가 커짐\n",
    "  - 그러나 코사인 유사도는 벡터의 크기와 상관없이 결과가 안정적이므로 좋은 결과를 기대할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f2cb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78ee0754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 계산\n",
    "def cos_sim(vec1, vec2):\n",
    "    return dot(vec1, vec2) / (norm(vec1) * norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91a0df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 계산 (다른 버전)\n",
    "def get_cosine_similarity(x1, x2):\n",
    "    return (x1 * x2).sum() / ((x1**2).sum()**.5 * (x2**2).sum()**.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0a033",
   "metadata": {},
   "source": [
    "단어 문서 행렬(Term-Document Matrix, TDM)이란 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c31e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TDM 만들기\n",
    "def make_term_doc_mat(sentence_bow, word_dics):\n",
    "    freq_mat = {}\n",
    "\n",
    "    for word in word_dics:\n",
    "        freq_mat[word] = 0\n",
    "\n",
    "    for word in word_dics:\n",
    "        if word in sentence_bow:\n",
    "            freq_mat[word] += 1\n",
    "\n",
    "    return freq_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b029e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 벡터 만들기\n",
    "def make_vector(tdm):\n",
    "    vec = []\n",
    "    for key in tdm:\n",
    "        vec.append(tdm[key])\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "976d3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 묶음 리스트를 하나로 합침\n",
    "bow = bow1 + bow2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73bf7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 묶음에서 중복제거해 단어 사전 구축\n",
    "word_dics = []\n",
    "for token in bow:\n",
    "    if token not in word_dics:\n",
    "        word_dics.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77f205bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'미': 1, '정상은': 1, '이번': 1, '회담': 1, '반도체': 1, '배터리': 1, '인공지능': 1, '등': 1, '핵심': 1, '신흥': 1, '기술': 1, '협력': 1, '안전': 1, '지속': 1, '회복': 1, '력': 1, '글로벌': 1, '공급': 1, '망': 1, '공조': 1, '윤': 0, '대통령': 0, '바이든': 0, '세계': 0, '최대': 0, '생산': 0, '시설': 0, '삼성': 0, '평택': 0, '캠퍼스': 0, '시찰': 0, '동맹': 0, '행보': 0}\n",
      "{'미': 0, '정상은': 0, '이번': 0, '회담': 0, '반도체': 1, '배터리': 0, '인공지능': 0, '등': 0, '핵심': 0, '신흥': 0, '기술': 0, '협력': 0, '안전': 0, '지속': 0, '회복': 0, '력': 0, '글로벌': 0, '공급': 0, '망': 0, '공조': 0, '윤': 1, '대통령': 1, '바이든': 1, '세계': 1, '최대': 1, '생산': 1, '시설': 1, '삼성': 1, '평택': 1, '캠퍼스': 1, '시찰': 1, '동맹': 1, '행보': 1}\n"
     ]
    }
   ],
   "source": [
    "# 문장 별 단어 문서 행렬 계산\n",
    "freq_list1 = make_term_doc_mat(bow1, word_dics)\n",
    "freq_list2 = make_term_doc_mat(bow2, word_dics)\n",
    "\n",
    "print(freq_list1)\n",
    "print(freq_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b4c09fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05976143046671968\n"
     ]
    }
   ],
   "source": [
    "# 코사인 유사도 계산\n",
    "doc1 = np.array(make_vector(freq_list1))\n",
    "doc2 = np.array(make_vector(freq_list2))\n",
    "\n",
    "r1 = cos_sim(doc1, doc2)\n",
    "print(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c577527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05976143046671968\n"
     ]
    }
   ],
   "source": [
    "r1 = get_cosine_similarity(doc1, doc2)\n",
    "\n",
    "print(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d22db99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사 단어 랭킹 표시 \n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57a245ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    '''동시발생 행렬 생성\n",
    "\n",
    "    :param corpus: 말뭉치(단어 ID 목록)\n",
    "    :param vocab_size: 어휘 수\n",
    "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
    "    :return: 동시발생 행렬\n",
    "    '''\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f84e81d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 계산 (또 다른 버전)\n",
    "def cos_similarity(x, y):\n",
    "  nx = x / np.sqrt(np.sum(x ** 2))\n",
    "  ny = y / np.sqrt(np.sum(y ** 2))\n",
    "  return np.dot(nx, ny)\n",
    "\n",
    "# 파라미터에 제로벡터가 들어오면 \"Divided by Zero\"오류 발생.\n",
    "# 매우 작은 수 eps를 분모에 추가하여 회피\n",
    "def new_cos_similarity(x, y, eps=1e-8):\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4672b921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067811865475\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "c0 = C[word_to_id['you']]  # \"you\"의 단어 벡터\n",
    "c1 = C[word_to_id['i']]    # \"i\"의 단어 벡터\n",
    "print(cos_similarity(c0, c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7dd070c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    '''유사 단어 검색\n",
    "\n",
    "    :param query: 쿼리(텍스트)\n",
    "    :param word_to_id: 단어에서 단어 ID로 변환하는 딕셔너리\n",
    "    :param id_to_word: 단어 ID에서 단어로 변환하는 딕셔너리\n",
    "    :param word_matrix: 단어 벡터를 정리한 행렬. 각 행에 해당 단어 벡터가 저장되어 있다고 가정한다.\n",
    "    :param top: 상위 몇 개까지 출력할 지 지정\n",
    "    '''\n",
    "    if query not in word_to_id:\n",
    "        print('%s(을)를 찾을 수 없습니다.' % query)\n",
    "        return\n",
    "\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    # 코사인 유사도를 기준으로 내림차순으로 출력\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45015b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " goodbye: 0.7071067811865475\n",
      " i: 0.7071067811865475\n",
      " hello: 0.7071067811865475\n",
      " say: 0.0\n",
      " and: 0.0\n"
     ]
    }
   ],
   "source": [
    "most_similar('you', word_to_id, id_to_word, C, top=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
