{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fed49cd",
   "metadata": {},
   "source": [
    "# **CNN을 이용한 자연어 처리**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907c58fa",
   "metadata": {},
   "source": [
    "---\n",
    "- 참고도서\n",
    "  - 파이토치로 배우는 자연어처리(델립라오, 브라이언 맥머핸 지음 / 박해선 옮김 | 한빛미디어)\n",
    "  - 텐서플로 2와 머신러닝으로 시작하는 자연어 처리 (전창욱, 최태균, 조종현, 신성진 지음 | 위키북스)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24601160",
   "metadata": {},
   "source": [
    "## PyTorch로 CNN 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838df9a",
   "metadata": {},
   "source": [
    "#### 데이터 만들기\n",
    "- 특성 벡터를 만들기 위하여 실제 데이터와 크기가 같은 3차원의 인공 데이터 텐서 생성\n",
    "- 파이토치의 Conv1d 클래스의 객체를 생성한 3차원 데이터 텐서에 적용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "870fd19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d644e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25849475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6451,  0.0804,  0.4231,  0.1118,  1.3436, -0.3778, -0.4332],\n",
       "         [-0.4591, -0.3115,  0.1800,  0.1065,  0.7512, -0.1620, -0.2731],\n",
       "         [-0.0486,  0.5535,  1.0949, -3.3168,  1.0957, -2.5783, -1.1879],\n",
       "         [ 0.3035,  0.2791,  1.1428, -1.1389,  2.8446,  0.9401, -0.0875],\n",
       "         [-1.0267,  0.8186,  0.4712, -0.1381, -1.9639,  0.5746, -0.6764],\n",
       "         [ 0.8685, -0.3459, -1.6355,  1.2856, -0.8374,  3.2893, -0.9696],\n",
       "         [ 0.9074,  0.9320, -1.0239,  0.7372,  0.1891, -0.1642,  0.0505],\n",
       "         [ 0.9899,  0.3232, -0.8959, -1.3977,  0.8550, -1.0668,  1.3817],\n",
       "         [-1.1231,  1.7758, -0.2828,  0.4760, -0.2252, -0.9463, -1.3306],\n",
       "         [-1.2084, -0.6370,  2.7545,  0.5976,  1.1054,  1.1608,  0.2737]],\n",
       "\n",
       "        [[-0.6912,  0.0220,  0.5199,  0.1173,  1.4279, -0.5334,  1.2145],\n",
       "         [-1.5768, -1.5994, -0.0970,  0.5190,  0.8221, -1.2244,  1.0968],\n",
       "         [-0.3024, -1.0748, -0.7415, -0.7627,  0.0148,  0.2493,  1.3945],\n",
       "         [ 0.7982, -1.7992, -1.7755,  1.2842, -1.1339,  1.1470, -0.6743],\n",
       "         [ 0.1356, -0.8627, -0.1634, -0.1728, -1.3305,  1.5155, -0.0436],\n",
       "         [ 2.6601, -0.1519, -0.0725, -0.4248,  0.4293,  0.3809,  0.2121],\n",
       "         [ 0.2971,  0.7947, -1.1869,  0.6882, -0.6084, -0.9641,  0.4729],\n",
       "         [-0.7994, -0.5077, -0.3654,  0.4876, -1.9492,  0.1095, -1.6647],\n",
       "         [ 0.1060,  0.4971, -1.1131,  0.1745,  1.2281,  1.0197, -0.9230],\n",
       "         [-1.1120,  0.7535, -1.1679, -0.1788, -1.2506, -0.2163,  0.6453]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2 \n",
    "one_hot_size = 10\n",
    "sequence_width = 7\n",
    "\n",
    "data = torch.randn(batch_size, one_hot_size, sequence_width)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4464e0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 7])\n",
      "torch.Size([2, 16, 5])\n"
     ]
    }
   ],
   "source": [
    "#  1D convolution 연산은 가로로만 이동하면서 output을 계산\n",
    "conv1 = nn.Conv1d(in_channels=one_hot_size, out_channels=16, kernel_size=3)\n",
    "intermediate1 = conv1(data)\n",
    "\n",
    "print(data.size())\n",
    "print(intermediate1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d2d0f",
   "metadata": {},
   "source": [
    "Parameters\n",
    "\n",
    "* in_channels: input의 feature dimension\n",
    "* out_channels: output으로 내고싶은 dimension\n",
    "* kernel_size: time step을 얼마만큼 볼 것인가(=frame size = filter size)\n",
    "* stride: kernel을 얼마만큼씩 이동하면서 적용할 것인가 (Default: 1) -> 아래 추가 설명\n",
    "* dilation: kernel 내부에서 얼마만큼 띄어서 kernel을 적용할 것인가 (Default: 1) -> 아래 추가 설명\n",
    "* padding: 한 쪽 방향으로 얼마만큼 padding할 것인가 (그 만큼 양방향으로 적용) (Default: 0)\n",
    "* groups: kernel의 height를 조절 (Default: 1) -> 아래 추가 설명\n",
    "* bias: bias term을 둘 것인가 안둘 것인가 (Default: True)\n",
    "* padding_mode: 'zeros', 'reflect', 'reflect', 'replicate', 'circular' (Default: 'zeros')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d9843",
   "metadata": {},
   "source": [
    "- 데이터에 합성곱 반복 적용\n",
    "  - 합성곱을 추가하여 출력 텐서의 크기를 줄이는 작업을 반복 적용\n",
    "  - 코드에서는 3번의 합성곱 후에 출력의 마지막 차원이 size=1이 되도록 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c19e7de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 3])\n",
      "torch.Size([2, 64, 1])\n"
     ]
    }
   ],
   "source": [
    "conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "\n",
    "intermediate2 = conv2(intermediate1)\n",
    "intermediate3 = conv3(intermediate2)\n",
    "\n",
    "print(intermediate2.size())\n",
    "print(intermediate3.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a1c4447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "y_output = intermediate3.squeeze()\n",
    "print(y_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf2a6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate2.mean(dim=0).mean(dim=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce2286d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 80])\n",
      "torch.Size([2, 16])\n"
     ]
    }
   ],
   "source": [
    "# 특성 벡터를 줄이는 방법 1\n",
    "print(intermediate1.view(batch_size, -1).size())\n",
    "\n",
    "# 특성 벡터를 줄이는 방법 2\n",
    "print(torch.mean(intermediate1, dim=2).size())\n",
    "# print(torch.max(intermediate1, dim=2).size())\n",
    "# print(torch.sum(intermediate1, dim=2).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7af55f",
   "metadata": {},
   "source": [
    "- 배치 정규화와 Conv1D 층 사용하기\n",
    "  - 전체 모델을 다시 만들지 않고 배치 정규화를 사용하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d631eaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 5])\n",
      "torch.Size([2, 32, 3])\n",
      "torch.Size([2, 64, 1])\n"
     ]
    }
   ],
   "source": [
    "conv1 = nn.Conv1d(in_channels=one_hot_size, out_channels=16, kernel_size=3)\n",
    "conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "\n",
    "conv1_bn = nn.BatchNorm1d(num_features=16)\n",
    "conv2_bn = nn.BatchNorm1d(num_features=32)\n",
    "    \n",
    "intermediate1 = conv1_bn(F.relu(conv1(data)))\n",
    "intermediate2 = conv2_bn(F.relu(conv2(intermediate1)))\n",
    "intermediate3 = conv3(intermediate2)\n",
    "\n",
    "print(intermediate1.size())\n",
    "print(intermediate2.size())\n",
    "print(intermediate3.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c33742d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 노트: \n",
    "# 배치 정규화는 배치와 시퀀스 차원에 대해 통곗값을 계산합니다. \n",
    "# 다른 말로하면 BatchNorm1d에 입력되는 텐서의 크기는 (B, C, L)입니다(B는 배치, C는 채널, L은 길이).\n",
    "# 각 (B, L) 슬라이스마다 원점에 평균을 맞춥니다. \n",
    "# 이는 공변량 변화(covariate shift)를 줄입니다.\n",
    "\n",
    "intermediate2.mean(dim=(0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65060a3f",
   "metadata": {},
   "source": [
    "- 여러 하이퍼파라미터 설정으로 합성곱 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb6ee49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(x):\n",
    "    print(\"타입: {}\".format(x.type()))\n",
    "    print(\"크기: {}\".format(x.shape))\n",
    "    print(\"값: \\n{}\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59452e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "타입: torch.FloatTensor\n",
      "크기: torch.Size([1, 2, 3, 3])\n",
      "값: \n",
      "tensor([[[[ 0.8558, -0.6681,  0.7447],\n",
      "          [ 0.0327,  0.3702,  0.5227],\n",
      "          [-0.6623, -0.3958, -0.7486]],\n",
      "\n",
      "         [[-0.1132,  1.0427, -1.4462],\n",
      "          [ 1.4887, -2.9881,  0.1496],\n",
      "          [-0.5606, -1.0756,  0.6233]]]])\n",
      "타입: torch.FloatTensor\n",
      "크기: torch.Size([1, 2, 2, 2])\n",
      "값: \n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0276,  0.2742],\n",
      "          [-0.1092, -0.1691]],\n",
      "\n",
      "         [[-0.1955,  0.0439],\n",
      "          [-0.1660, -0.2106]]]], requires_grad=True)\n",
      "타입: torch.FloatTensor\n",
      "크기: torch.Size([1, 1, 2, 2])\n",
      "값: \n",
      "tensor([[[[0.2704, 0.3003],\n",
      "          [0.1852, 1.0075]]]], grad_fn=<ThnnConv2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 2, 3, 3)\n",
    "describe(x)\n",
    "\n",
    "conv1 = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=2)\n",
    "describe(conv1.weight)\n",
    "describe(conv1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c65e2bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "타입: torch.FloatTensor\n",
      "크기: torch.Size([1, 1, 3, 3])\n",
      "값: \n",
      "tensor([[[[ 1.0448,  0.5967,  0.8600],\n",
      "          [-1.1469,  0.9322, -0.3154],\n",
      "          [-1.7356, -1.0824, -0.1431]]]])\n",
      "타입: torch.FloatTensor\n",
      "크기: torch.Size([2, 1, 2, 2])\n",
      "값: \n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0461, -0.3873],\n",
      "          [ 0.0592, -0.4045]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1869, -0.0788],\n",
      "          [ 0.3234,  0.2475]]]], requires_grad=True)\n",
      "타입: torch.FloatTensor\n",
      "크기: torch.Size([1, 2, 2, 2])\n",
      "값: \n",
      "tensor([[[[-0.2854,  0.2197],\n",
      "          [ 0.2638,  0.5015]],\n",
      "\n",
      "         [[ 0.5013,  0.7605],\n",
      "          [-0.6238,  0.3068]]]], grad_fn=<ThnnConv2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 3, 3)\n",
    "describe(x)\n",
    "\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=2)\n",
    "describe(conv1.weight)\n",
    "describe(conv1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced2d98",
   "metadata": {},
   "source": [
    "## 자연어 처리에서 CNN\n",
    "- CNN을 이용한 문장 분류 아키텍처\n",
    "<img src='https://drive.google.com/uc?export=download&id=1Si0ma4WjKI0Hp02bTv7lizAYrTQtfQ7R' /><br>\n",
    "\n",
    "  - n개의 단어로 이루어진 리뷰 문장을 각 단어별로 k차원의 행벡터로 임베딩\n",
    "  - CNN 필터의 크기는 2, 3\n",
    "  - 필터 개수만큼의 Feature Map을 만들고 Max-Pooling 과정을 거쳐 클래스 개수(긍정 혹은 부정:2개)만큼의 스코어를 출력하는 네트워크 구조\n",
    "<br><br>\n",
    "- 자연어 처리에 사용되는 CNN\n",
    "<img src='https://drive.google.com/uc?export=download&id=1bxzttajTIt1cMFrNTlOVvwmUptfo9rZl' /><br>\n",
    "\n",
    "  - 이미지 처리에 사용되는 2D CNN과 달리 1D CNN 사용\n",
    "  - 1D CNN: 커널의 넓이를 문장 행렬에서의 임베딩 벡터의 차원과 동일하게 설정\n",
    "    - 예: 커널 사이즈 = 2 이면 높이가 2, 너비가 임베딩 벡터의 차원인 커널\n",
    "<br><br>\n",
    "- 논문: http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8d9cfa",
   "metadata": {},
   "source": [
    "## CNN으로 성씨 분류하기(PyTorch)\n",
    "- 신경망 설계의 목적: 작업을 달성할 하이퍼 파라미터 설정을 찾는 것\n",
    "- 구현하는 CNN 모델의 마지막 층은 Linear층 적용\n",
    "  - 마지막의 Linear 층은 일련의 합성곱 층이 만든 특성 벡터에서 예측 벡터를 만드는 법을 학습함\n",
    "  - 원하는 특성 벡터를 만드는 합성곱 층을 구성하는 것이 목적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "278a2f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb4964af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surname</th>\n",
       "      <th>nationality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Woodford</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coté</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kore</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Koury</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lebzak</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    surname nationality\n",
       "0  Woodford     English\n",
       "1      Coté      French\n",
       "2      Kore     English\n",
       "3     Koury      Arabic\n",
       "4    Lebzak     Russian"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surname = pd.read_csv('LectureData/data/surnames/surnames.csv', header = 0)\n",
    "surname_split = pd.read_csv('LectureData/data/surnames/surnames_with_splits.csv', header = 0)\n",
    "\n",
    "surname.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "733a916f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arabic',\n",
       " 'Chinese',\n",
       " 'Czech',\n",
       " 'Dutch',\n",
       " 'English',\n",
       " 'French',\n",
       " 'German',\n",
       " 'Greek',\n",
       " 'Irish',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Korean',\n",
       " 'Polish',\n",
       " 'Portuguese',\n",
       " 'Russian',\n",
       " 'Scottish',\n",
       " 'Spanish',\n",
       " 'Vietnamese'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(surname.nationality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5566949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 국적 기반으로 전처리\n",
    "\n",
    "# collections.defaultdict는 딕셔너리(dictionary)와 거의 비슷하지만\n",
    "#key값이 없을 경우 미리 지정해 놓은 초기(default)값을 반환하는 dictionary\n",
    "\n",
    "by_nationality = collections.defaultdict(list)\n",
    "\n",
    "# Iterate over DataFrame rows as (index, Series) pairs.\n",
    "for _, row in surname.iterrows() :\n",
    "    by_nationality[row.nationality].append(row.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95429761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할 \n",
    "\n",
    "train_proportion=0.7\n",
    "val_proportion=0.15\n",
    "test_proportion=0.15\n",
    "\n",
    "final_list = []\n",
    "np.random.seed(1)\n",
    "\n",
    "for _, item_list in sorted(by_nationality.items()) :\n",
    "    np.random.shuffle(item_list)\n",
    "    n = len(item_list)\n",
    "    n_train = int(train_proportion*n)\n",
    "    n_val = int(val_proportion*n)\n",
    "    n_test = int(test_proportion*n)\n",
    "    \n",
    "    \n",
    "\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "    for item in item_list[n_train+n_val:]:\n",
    "        item['split'] = 'test'  \n",
    "    \n",
    "    \n",
    "    final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d46d2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surname</th>\n",
       "      <th>nationality</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nassar</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Awad</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ghanem</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Isa</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Samaha</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  surname nationality  split\n",
       "0  Nassar      Arabic  train\n",
       "1    Awad      Arabic  train\n",
       "2  Ghanem      Arabic  train\n",
       "3     Isa      Arabic  train\n",
       "4  Samaha      Arabic  train"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_surnames = pd.DataFrame(final_list)\n",
    "final_surnames.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cf3db0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    7680\n",
       "test     1660\n",
       "val      1640\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_surnames.split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e61245d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_surnames.to_csv('LectureData/data/surnames/surnames_with_splits.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a06782db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d2ce7",
   "metadata": {},
   "source": [
    "### 데이터 벡터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81f4de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"매핑을 위해 텍스트를 처리하고 어휘 사전을 만드는 클래스\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            token_to_idx (dict): 기존 토큰-인덱스 매핑 딕셔너리\n",
    "            add_unk (bool): UNK 토큰을 추가할지 지정하는 플래그\n",
    "            unk_token (str): Vocabulary에 추가할 UNK 토큰\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "        \n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\" 직렬화할 수 있는 딕셔너리를 반환합니다 \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" 직렬화된 딕셔너리에서 Vocabulary 객체를 만듭니다 \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\" 토큰을 기반으로 매핑 딕셔너리를 업데이트합니다\n",
    "\n",
    "        매개변수:\n",
    "            token (str): Vocabulary에 추가할 토큰\n",
    "        반환값:\n",
    "            index (int): 토큰에 상응하는 정수\n",
    "        \"\"\"\n",
    "        try:\n",
    "            index = self._token_to_idx[token]\n",
    "        except KeyError:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\" 토큰 리스트를 Vocabulary에 추가합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            tokens (list): 문자열 토큰 리스트\n",
    "        반환값:\n",
    "            indices (list): 토큰 리스트에 상응되는 인덱스 리스트\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\" 토큰에 대응하는 인덱스를 추출합니다.\n",
    "        토큰이 없으면 UNK 인덱스를 반환합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            token (str): 찾을 토큰 \n",
    "        반환값:\n",
    "            index (int): 토큰에 해당하는 인덱스\n",
    "        노트:\n",
    "            UNK 토큰을 사용하려면 (Vocabulary에 추가하기 위해)\n",
    "            `unk_index`가 0보다 커야 합니다.\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\" 인덱스에 해당하는 토큰을 반환합니다.\n",
    "        \n",
    "        매개변수: \n",
    "            index (int): 찾을 인덱스\n",
    "        반환값:\n",
    "            token (str): 인텍스에 해당하는 토큰\n",
    "        에러:\n",
    "            KeyError: 인덱스가 Vocabulary에 없을 때 발생합니다.\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d7fe78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    \"\"\" 어휘 사전을 생성하고 관리합니다 \"\"\"\n",
    "    def __init__(self, surname_vocab, nationality_vocab, max_surname_length):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            surname_vocab (Vocabulary): 문자를 정수에 매핑하는 Vocabulary 객체\n",
    "            nationality_vocab (Vocabulary): 국적을 정수에 매핑하는 Vocabulary 객체\n",
    "            max_surname_length (int): 가장 긴 성씨 길이\n",
    "        \"\"\"\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "        self._max_surname_length = max_surname_length\n",
    "\n",
    "    def vectorize(self, surname):\n",
    "        \"\"\" 성씨에 대한 원-핫 벡터를 만듭니다\n",
    "\n",
    "        매개변수:\n",
    "            surname (str): 성씨\n",
    "        반환값:\n",
    "            one_hot (np.ndarray): 원-핫 벡터의 행렬\n",
    "        \"\"\"\n",
    "        one_hot_matrix_size = (len(self.surname_vocab), self._max_surname_length)\n",
    "        one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)\n",
    "                               \n",
    "        for position_index, character in enumerate(surname):\n",
    "            character_index = self.surname_vocab.lookup_token(character)\n",
    "            one_hot_matrix[character_index][position_index] = 1\n",
    "        \n",
    "        return one_hot_matrix\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        \"\"\" 데이터셋 데이터프레임에서 Vectorizer 객체를 만듭니다\n",
    "        \n",
    "        매개변수:\n",
    "            surname_df (pandas.DataFrame): 성씨 데이터셋\n",
    "        반환값:\n",
    "            SurnameVectorizer 객체\n",
    "        \"\"\"\n",
    "        surname_vocab = Vocabulary(unk_token=\"@\")\n",
    "        nationality_vocab = Vocabulary(add_unk=False)\n",
    "        max_surname_length = 0\n",
    "\n",
    "        for index, row in surname_df.iterrows():\n",
    "            max_surname_length = max(max_surname_length, len(row.surname))\n",
    "            for letter in row.surname:\n",
    "                surname_vocab.add_token(letter)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "\n",
    "        return cls(surname_vocab, nationality_vocab, max_surname_length)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        surname_vocab = Vocabulary.from_serializable(contents['surname_vocab'])\n",
    "        nationality_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
    "        return cls(surname_vocab=surname_vocab, nationality_vocab=nationality_vocab, \n",
    "                   max_surname_length=contents['max_surname_length'])\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'surname_vocab': self.surname_vocab.to_serializable(),\n",
    "                'nationality_vocab': self.nationality_vocab.to_serializable(), \n",
    "                'max_surname_length': self._max_surname_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bad76046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            surname_df (pandas.DataFrame): 데이터셋\n",
    "            vectorizer (SurnameVectorizer): SurnameVectorizer 객체\n",
    "        \"\"\"\n",
    "        self.surname_df = surname_df\n",
    "        self.vectorizer = vectorizer\n",
    "        self.train_df = self.surname_df[self.surname_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.surname_df[self.surname_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.surname_df[self.surname_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "        \n",
    "        # 클래스 가중치\n",
    "        class_counts = surname_df.nationality.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self.vectorizer.nationality_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
    "        \"\"\" 데이터셋을 로드하고 새로운 SurnameVectorizer 객체를 만듭니다\n",
    "        \n",
    "        매개변수:\n",
    "            review_csv (str): 데이터셋의 위치\n",
    "        반환값:\n",
    "            SurnameDataset의 인스턴스\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        train_surname_df = surname_df[surname_df.split=='train']\n",
    "        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\n",
    "        \"\"\"데이터셋을 로드하고 새로운 SurnameVectorizer 객체를 만듭니다.\n",
    "        캐시된 SurnameVectorizer 객체를 재사용할 때 사용합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            surname_csv (str): 데이터셋의 위치\n",
    "            vectorizer_filepath (str): SurnameVectorizer 객체의 저장 위치\n",
    "        반환값:\n",
    "            SurnameDataset의 인스턴스\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(surname_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"파일에서 SurnameVectorizer 객체를 로드하는 정적 메서드\n",
    "        \n",
    "        매개변수:\n",
    "            vectorizer_filepath (str): 직렬화된 SurnameVectorizer 객체의 위치\n",
    "        반환값:\n",
    "            SurnameVectorizer의 인스턴스\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\" SurnameVectorizer 객체를 json 형태로 디스크에 저장합니다\n",
    "        \n",
    "        매개변수:\n",
    "            vectorizer_filepath (str): SurnameVectorizer 객체의 저장 위치\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._ectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" 벡터 변환 객체를 반환합니다 \"\"\"\n",
    "        return self.vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" 데이터프레임에 있는 열을 사용해 분할 세트를 선택합니다 \n",
    "        \n",
    "        매개변수:\n",
    "            split (str): \"train\", \"val\", \"test\" 중 하나\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" 파이토치 데이터셋의 주요 진입 메서드\n",
    "        \n",
    "        매개변수:\n",
    "            index (int): 데이터 포인트의 인덱스\n",
    "        반환값:\n",
    "            데이터 포인트의 특성(x_surname)과 레이블(y_nationality)로 이루어진 딕셔너리\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        surname_matrix = \\\n",
    "            self.vectorizer.vectorize(row.surname)\n",
    "\n",
    "        nationality_index = \\\n",
    "            self.vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "\n",
    "        return {'x_surname': surname_matrix,\n",
    "                'y_nationality': nationality_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\" 배치 크기가 주어지면 데이터셋으로 만들 수 있는 배치 개수를 반환합니다\n",
    "        \n",
    "        매개변수:\n",
    "            batch_size (int)\n",
    "        반환값:\n",
    "            배치 개수\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73fcb777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    파이토치 DataLoader를 감싸고 있는 제너레이터 함수.\n",
    "    걱 텐서를 지정된 장치로 이동합니다.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3c491",
   "metadata": {},
   "source": [
    "### 모델구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c559d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameClassifier(nn.Module):\n",
    "    def __init__(self, initial_num_channels, num_classes, num_channels):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            initial_num_channels (int): 입력 특성 벡터의 크기\n",
    "            num_classes (int): 출력 예측 벡터의 크기\n",
    "            num_channels (int): 신경망 전체에 사용될 채널 크기\n",
    "        \"\"\"\n",
    "        super(SurnameClassifier, self).__init__()\n",
    "        \n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=initial_num_channels, \n",
    "                      out_channels=num_channels, kernel_size=3),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                      kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                      kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                      kernel_size=3),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.fc = nn.Linear(num_channels, num_classes)\n",
    "\n",
    "    def forward(self, x_surname, apply_softmax=False):\n",
    "        \"\"\"모델의 정방향 계산\n",
    "        \n",
    "        매개변수:\n",
    "            x_surname (torch.Tensor): 입력 데이터 텐서. \n",
    "                x_surname.shape은 (batch, initial_num_channels, max_surname_length)입니다.\n",
    "            apply_softmax (bool): 소프트맥스 활성화 함수를 위한 플래그\n",
    "                크로스-엔트로피 손실을 사용하려면 False로 지정해야 합니다.\n",
    "        반환값:\n",
    "            결과 텐서. tensor.shape은 (batch, num_classes)입니다.\n",
    "        \"\"\"\n",
    "        features = self.convnet(x_surname).squeeze(dim=2)\n",
    "       \n",
    "        prediction_vector = self.fc(features)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe46b6e7",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2090e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b118a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\" 훈련 상태를 업데이트합니다.\n",
    "\n",
    "    Components:\n",
    "     - 조기 종료: 과대 적합 방지\n",
    "     - 모델 체크포인트: 더 나은 모델을 저장합니다\n",
    "\n",
    "    :param args: 메인 매개변수\n",
    "    :param model: 훈련할 모델\n",
    "    :param train_state: 훈련 상태를 담은 딕셔너리\n",
    "    :returns:\n",
    "        새로운 훈련 상태\n",
    "    \"\"\"\n",
    "    \n",
    "    # 적어도 한 번 모델을 저장합니다\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # 성능이 향상되면 모델을 저장합니다\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # 손실이 나빠지면\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # 조기 종료 단계 업데이트\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # 손실이 감소하면\n",
    "        else:\n",
    "            # 최상의 모델 저장\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # 조기 종료 단계 재설정\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # 조기 종료 여부 확인\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12ec6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_pred_indices = y_pred.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0bb333bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 경로: \n",
      "\tmodel_storage/cnn/vectorizer.json\n",
      "\tmodel_storage/cnn/model.pth\n",
      "CUDA 사용여부: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # 날짜와 경로 정보\n",
    "    surname_csv=\"LectureData/data/surnames/surnames_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/cnn\",\n",
    "    # 모델 하이퍼파라미터\n",
    "    hidden_dim=100,\n",
    "    num_channels=256,\n",
    "    # 훈련 하이퍼파라미터\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=128,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    dropout_p=0.1,\n",
    "    # 실행 옵션\n",
    "    cuda=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    catch_keyboard_interrupt=True\n",
    ")\n",
    "\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"파일 경로: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# CUDA 체크\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"CUDA 사용여부: {}\".format(args.cuda))\n",
    "\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "        \n",
    "# 재현성을 위해 시드 설정\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# 디렉토리 처리\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b446f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu로 학습하는데 약 17분 걸림 \n",
    "\n",
    "if args.reload_from_files:\n",
    "    # 체크포인트에서 훈련을 다시 시작\n",
    "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
    "                                                              args.vectorizer_file)\n",
    "else:\n",
    "    # 데이터셋과 Vectorizer 만들기\n",
    "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "    \n",
    "avectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = SurnameClassifier(initial_num_channels=len(vectorizer.surname_vocab), \n",
    "                               num_classes=len(vectorizer.nationality_vocab),\n",
    "                               num_channels=args.num_channels)\n",
    "\n",
    "classifer = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(weight=dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c788085",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_bar = tqdm.notebook.tqdm(desc='training routine', \n",
    "                               total=args.num_epochs,\n",
    "                               position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm.notebook.tqdm(desc='split=train',\n",
    "                               total=dataset.get_num_batches(args.batch_size), \n",
    "                               position=1, \n",
    "                               leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm.notebook.tqdm(desc='split=val',\n",
    "                             total=dataset.get_num_batches(args.batch_size), \n",
    "                             position=1, \n",
    "                             leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # 훈련 세트에 대한 순회\n",
    "\n",
    "        # 훈련 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # 훈련 과정은 5단계로 이루어집니다\n",
    "\n",
    "            # --------------------------------------\n",
    "            # 단계 1. 그레이디언트를 0으로 초기화합니다\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 단계 2. 출력을 계산합니다\n",
    "            y_pred = classifier(batch_dict['x_surname'])\n",
    "\n",
    "            # 단계 3. 손실을 계산합니다\n",
    "            loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 단계 4. 손실을 사용해 그레이디언트를 계산합니다\n",
    "            loss.backward()\n",
    "\n",
    "            # 단계 5. 옵티마이저로 가중치를 업데이트합니다\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "\n",
    "            # 정확도를 계산합니다\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # 진행 바 업데이트\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # 검증 세트에 대한 순회\n",
    "\n",
    "        # 검증 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # 단계 1. 출력을 계산합니다\n",
    "            y_pred =  classifier(batch_dict['x_surname'])\n",
    "\n",
    "            # 단계 2. 손실을 계산합니다\n",
    "            loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 단계 3. 정확도를 계산합니다\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34250b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # 출력을 계산합니다\n",
    "    y_pred =  classifier(batch_dict['x_surname'])\n",
    "    \n",
    "    # 손실을 계산합니다\n",
    "    loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # 정확도를 계산합니다\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f39c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"테스트 손실: {};\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78552e81",
   "metadata": {},
   "source": [
    "### 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5d1cd19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nationality(surname, classifier, vectorizer):\n",
    "    \"\"\"새로운 성씨로 국적 예측하기\n",
    "    \n",
    "    매개변수:\n",
    "        surname (str): 분류할 성씨\n",
    "        classifier (SurnameClassifer): 분류기 객체\n",
    "        vectorizer (SurnameVectorizer): SurnameVectorizer 객체\n",
    "    반환값:\n",
    "        가장 가능성이 높은 국적과 확률로 구성된 딕셔너리\n",
    "    \"\"\"\n",
    "    vectorized_surname = vectorizer.vectorize(surname)\n",
    "    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(0)\n",
    "    result = classifier(vectorized_surname, apply_softmax=True)\n",
    "\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    index = indices.item()\n",
    "\n",
    "    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "    probability_value = probability_values.item()\n",
    "\n",
    "    return {'nationality': predicted_nationality, 'probability': probability_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd60335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_surname = input(\"분류하려는 성씨를 입력하세요: \")\n",
    "classifier = classifier.cpu()\n",
    "prediction = predict_nationality(new_surname, classifier, vectorizer)\n",
    "print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
    "                                    prediction['nationality'],\n",
    "                                    prediction['probability']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafbb939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topk_nationality(surname, classifier, vectorizer, k=5):\n",
    "    \"\"\"새로운 성씨에 대한 최상위 K개 국적을 예측합니다\n",
    "    \n",
    "    매개변수:\n",
    "        surname (str): 분류하려는 성씨\n",
    "        classifier (SurnameClassifer): 분류기 객체\n",
    "        vectorizer (SurnameVectorizer): SurnameVectorizer 객체\n",
    "        k (int): the number of top nationalities to return\n",
    "    반환값:\n",
    "        딕셔너리 리스트, 각 딕셔너리는 국적과 확률로 구성됩니다.\n",
    "    \"\"\"\n",
    "    vectorized_surname = vectorizer.vectorize(surname)\n",
    "    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(dim=0)\n",
    "    prediction_vector = classifier(vectorized_surname, apply_softmax=True)\n",
    "    probability_values, indices = torch.topk(prediction_vector, k=k)\n",
    "    \n",
    "    # 반환되는 크기는 (1,k)입니다\n",
    "    probability_values = probability_values[0].detach().numpy()\n",
    "    indices = indices[0].detach().numpy()\n",
    "    \n",
    "    results = []\n",
    "    for kth_index in range(k):\n",
    "        nationality = vectorizer.nationality_vocab.lookup_index(indices[kth_index])\n",
    "        probability_value = probability_values[kth_index]\n",
    "        results.append({'nationality': nationality, \n",
    "                        'probability': probability_value})\n",
    "    return results\n",
    "\n",
    "new_surname = input(\"분류하려는 성씨를 입력하세요: \")\n",
    "\n",
    "k = int(input(\"얼마나 많은 예측을 보고 싶나요? \"))\n",
    "if k > len(vectorizer.nationality_vocab):\n",
    "    print(\"앗! 전체 국적 개수보다 큰 값을 입력했습니다. 모든 국적에 대한 예측을 반환합니다. :)\")\n",
    "    k = len(vectorizer.nationality_vocab)\n",
    "    \n",
    "predictions = predict_topk_nationality(new_surname, classifier, vectorizer, k=k)\n",
    "\n",
    "print(\"최상위 {}개 예측:\".format(k))\n",
    "print(\"===================\")\n",
    "for prediction in predictions:\n",
    "    print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
    "                                        prediction['nationality'],\n",
    "                                        prediction['probability']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
